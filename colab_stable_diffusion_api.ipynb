# Stable Diffusion API Server for Google Colab
# 
# This script runs in Google Colab with GPU acceleration.
# It loads Stable Diffusion and exposes it as a public API via ngrok.

# @title üîÑ Stable Diffusion API Server
# @markdown Run this cell to start the API server
# @markdown ### Instructions:
# @markdown 1. Run this cell in Google Colab with GPU enabled
# @markdown 2. Copy the ngrok URL shown below
# @markdown 3. Use the URL with the local client to generate images

# ============================================================================
# IMPORTS AND SETUP
# ============================================================================

import os
import sys
import io
import base64
import threading
import time
from datetime import datetime

# Install required packages
print("üì¶ Installing dependencies...")
os.system("pip install fastapi uvicorn pyngrok diffusers transformers accelerate safetensors pillow requests")

# ============================================================================
# CONFIGURATION
# ============================================================================

# Model configuration - using Stable Diffusion 1.5 for reliability
MODEL_ID = "runwayml/stable-diffusion-v1-5"
DEFAULT_HEIGHT = 512
DEFAULT_WIDTH = 512
DEFAULT_STEPS = 25
DEFAULT_GUIDANCE = 7.5

# ============================================================================
# MODEL LOADING
# ============================================================================

print("üöÄ Loading Stable Diffusion model...")
print("   This may take 2-5 minutes on first run...")

from diffusers import StableDiffusionPipeline
import torch

# Load the pipeline with GPU optimization
pipe = StableDiffusionPipeline.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    safety_checker=None,  # Disable for API usage
    requires_safety_checker=False,
)

# Move to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = pipe.to(device)

# Enable optimizations
if torch.cuda.is_available():
    pipe.enable_attention_slicing()
    pipe.enable_vae_slicing()

print(f"‚úÖ Model loaded successfully on {device}")

# ============================================================================
# FASTAPI APPLICATION
# ============================================================================

from fastapi import FastAPI, HTTPException, Form, Query
from fastapi.responses import Response, JSONResponse
from pydantic import BaseModel, Field
from typing import Optional

app = FastAPI(
    title="Stable Diffusion API",
    description="Public API for image generation using Stable Diffusion",
    version="1.0.0"
)

# Request/Response models
class GenerateRequest(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=500)
    negative_prompt: Optional[str] = None
    height: int = Field(default=DEFAULT_HEIGHT, ge=256, le=1024)
    width: int = Field(default=DEFAULT_WIDTH, ge=256, le=1024)
    num_inference_steps: int = Field(default=DEFAULT_STEPS, ge=1, le=100)
    guidance_scale: float = Field(default=DEFAULT_GUIDANCE, ge=1.0, le=20.0)
    seed: Optional[int] = None

class GenerateResponse(BaseModel):
    success: bool
    image_url: Optional[str] = None
    seed: Optional[int] = None
    inference_time: float
    message: str

# Health check endpoint
@app.get("/")
def root():
    return {
        "service": "Stable Diffusion API",
        "status": "running",
        "device": device,
        "model": MODEL_ID,
        "endpoints": {
            "generate": "/generate (POST)",
            "health": "/health",
            "docs": "/docs"
        }
    }

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "device": device,
        "model_loaded": True,
        "cuda_available": torch.cuda.is_available()
    }

# ============================================================================
# IMAGE GENERATION ENDPOINT
# ============================================================================

@app.post("/generate")
def generate_image(
    prompt: str = Form(..., min_length=1, max_length=500),
    negative_prompt: Optional[str] = Form(None),
    height: int = Form(DEFAULT_HEIGHT),
    width: int = Form(DEFAULT_WIDTH),
    num_inference_steps: int = Form(DEFAULT_STEPS),
    guidance_scale: float = Form(DEFAULT_GUIDANCE),
    seed: Optional[int] = Form(None)
):
    """
    Generate an image from a text prompt.
    
    Parameters:
    - prompt: Text description of the desired image
    - negative_prompt: Things to avoid in the image
    - height: Image height (256-1024)
    - width: Image width (256-1024)
    - num_inference_steps: Quality (1-100, higher = better but slower)
    - guidance_scale: How closely to follow prompt (1-20)
    - seed: Random seed for reproducibility
    """
    start_time = time.time()
    
    try:
        # Validate prompt
        if not prompt or not prompt.strip():
            raise HTTPException(status_code=400, detail="Prompt cannot be empty")
        
        # Set random seed if provided
        generator = None
        if seed is not None:
            generator = torch.Generator(device=device).manual_seed(seed)
        
        # Generate image
        print(f"üé® Generating: '{prompt}'")
        
        with torch.inference_mode():
            result = pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator
            )
        
        image = result.images[0]
        
        # Convert to bytes
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='PNG')
        img_bytes = img_byte_arr.getvalue()
        
        # Encode as base64 for response
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        
        inference_time = time.time() - start_time
        actual_seed = seed if seed is not None else int(time.time() * 1000) % (2**32)
        
        print(f"‚úÖ Generated in {inference_time:.2f}s (seed: {actual_seed})")
        
        return {
            "success": True,
            "image_base64": img_base64,
            "seed": actual_seed,
            "inference_time": inference_time,
            "message": "Image generated successfully"
        }
        
    except Exception as e:
        inference_time = time.time() - start_time
        print(f"‚ùå Error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

@app.post("/generate/json")
def generate_image_json(request: GenerateRequest):
    """JSON-based generation endpoint"""
    return generate_image(
        prompt=request.prompt,
        negative_prompt=request.negative_prompt,
        height=request.height,
        width=request.width,
        num_inference_steps=request.num_inference_steps,
        guidance_scale=request.guidance_scale,
        seed=request.seed
    )

# ============================================================================
# NGROK TUNNEL SETUP
# ============================================================================

from pyngrok import ngrok

print("\n" + "="*60)
print("üîó Setting up ngrok tunnel...")
print("="*60)

# Set your ngrok auth token here (optional, for longer sessions)
# ngrok.set_auth_token("YOUR_NGROK_AUTH_TOKEN")

# Create tunnel
public_url = ngrok.connect(8000, "http")
print(f"\nüéâ API Server is live at:")
print(f"   {public_url}")
print(f"\nüìù Use this URL in your local client")
print("="*60)

# Keep the server running
print("\n‚è≥ Server is running. Press Ctrl+C to stop.")
print("üìå Endpoints:")
print(f"   - Generate: POST {public_url}/generate")
print(f"   - Health:   GET  {public_url}/health")
print(f"   - Docs:     GET  {public_url}/docs")

import uvicorn

# Run the FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)
